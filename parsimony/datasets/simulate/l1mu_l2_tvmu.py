# -*- coding: utf-8 -*-
"""
Created on Fri Sep 27 14:47:48 2013

Copyright (c) 2013-2014, CEA/DSV/I2BM/Neurospin. All rights reserved.

@author:  Tommy Löfstedt
@email:   tommy.loefstedt@cea.fr
@license: BSD 3-clause.
"""
import numpy as np
from .grad import grad_l1mu
from .grad import grad_l2_squared
from .grad import grad_tvmu
from .utils import bisection_method

__all__ = ['load']


def load(l, k, g, beta, M, e, mu, snr=None, shape=None):
    """Returns data generated such that we know the exact solution.

    The data generated by this function is fit to the Linear regression + L1 +
    L2 + Smoothed total variation function, i.e.:

        f(b) = (1 / 2).|Xb - y|² + l.L1mu(b) + (k / 2).|b|² + g.TVmu(b),

    where L1mu is the smoothed L1 norm, |.|² is the squared L2 norm and TVmu is
    the smoothed total variation penalty.

    Parameters
    ----------
    l : The L1 regularisation parameter.

    k : The L2 regularisation parameter.

    g : The total variation regularisation parameter.

    beta : The regression vector to generate data from.

    M : The matrix to use when building data. This matrix carries the desired
            correlation structure of the generated data. The generated data
            will be a column-scaled version of this matrix.

    e : The error vector e = Xb - y. This vector carries the desired
            distribution of the residual.

    mu : The Nesterov smoothing regularisation parameter.

    snr : Signal-to-noise ratio between model and residual.

    shape : The underlying dimension of the regression vector, beta. E.g. the
            beta may represent an underlying 3D image. In that case the shape
            is a three-tuple with dimensions (Z, Y, X). If shape is not
            provided, the shape is set to (p,) where p is the dimension of
            beta.

    Returns
    -------
    X : The generated X matrix.

    y : The generated y vector.

    beta : The regression vector with the correct snr.
    """
    l = float(l)
    k = float(k)
    g = float(g)

    if shape is None:
        shape = (beta.shape[0],)

    if snr is not None:
        def f(x):
            X, y = _generate(l, k, g, x * beta, M, e, mu, shape)

#            print "norm(beta) = ", np.linalg.norm(beta)
#            print "norm(Xbeta) = ", np.linalg.norm(np.dot(X, beta))
#            print "norm(e) = ", np.linalg.norm(e)

            print("snr = %.5f = %.5f = |X.b| / |e| = %.5f / %.5f"
                  % (snr, np.linalg.norm(np.dot(X, x * beta))
                          / np.linalg.norm(e),
                     np.linalg.norm(np.dot(X, x * beta)), np.linalg.norm(e)))

            return (np.linalg.norm(np.dot(X, x * beta)) / np.linalg.norm(e)) \
                   - snr

        snr = bisection_method(f, low=0.0, high=np.sqrt(snr), maxiter=30)

        beta = beta * snr

    X, y = _generate(l, k, g, beta, M, e, mu, shape)

    return X, y, beta


def _generate(l, k, g, beta, M, e, mu, shape):

    p = np.prod(shape)

    gradL1mu = grad_l1mu(beta, mu)
    gradL2 = grad_l2_squared(beta)
    gradTVmu = grad_tvmu(beta, shape, mu)

    alpha = -(l * gradL1mu + k * gradL2 + g * gradTVmu)
    alpha = np.divide(alpha, np.dot(M.T, e))

    X = np.zeros(M.shape)
    for i in range(p):
        X[:, i] = M[:, i] * alpha[i, 0]

    y = np.dot(X, beta) - e

    return X, y
