# -*- coding: utf-8 -*-
"""
Created on Thu May  8 10:49:38 2014

Copyright (c) 2013-2014, CEA/DSV/I2BM/Neurospin. All rights reserved.

@author:  Tommy Löfstedt
@email:   tommy.loefstedt@cea.fr
@license: BSD 3-clause.
"""
import numpy as np
from .grad import grad_l1
from .grad import grad_l2_squared
from .grad import grad_grouptvmu
from .utils import bisection_method

__all__ = ["load"]


def load(l, k, g, beta, M, e, A, mu, snr=None, intercept=False):
    """Returns data generated such that we know the exact solution.

    The data generated by this function is fit to the Linear regression + L1 +
    L2 + Smoothed group total variation function, i.e.:

        f(b) = (1 / 2).|Xb - y|² + l.|b|_1 + (k / 2).|b|² + g.GroupTVmu(b),

    where |.|_1 is the L1 norm, |.|² is the squared L2 norm and TVmu is the
    smoothed total variation penalty.

    Parameters
    ----------
    l : Non-negative float. The L1 regularisation parameter.

    k : Non-negative float. The L2 regularisation parameter.

    g : Non-negative float. The group total variation regularisation parameter.

    beta : Numpy array (p-by-1). The regression vector to generate data from.

    M : Numpy array (n-by-p). The matrix to use when building data. This
            matrix carries the desired correlation structure of the generated
            data. The generated data will be a column-scaled version of this
            matrix.

    e : Numpy array (n-by-1). The error vector e = Xb - y. This vector carries
            the desired distribution of the residual.

    A : Numpy or (usually) scipy.sparse array (K-by-p). The linear operator
            for the Nesterov function.

    mu : Positive float. The Nesterov smoothing regularisation parameter.

    snr : Positive float. Signal-to-noise ratio between model and residual.

    intercept : Boolean. Whether or not to include an intercept variable. This
            variable is not penalised. Note that if intercept is True, then e
            will be centred.

    Returns
    -------
    X : Numpy array (n-by-p). The generated X matrix.

    y : Numpy array (n-by-1). The generated y vector.

    beta : Numpy array (p-by-1). The regression vector with the correct snr.
    """
    l = float(l)
    k = float(k)
    g = float(g)

    if intercept:
        e = e - np.mean(e)

    if snr is not None:
        def f(x):
            X, y = _generate(l, k, g, x * beta, M, e, A, mu, intercept)

#            print "snr = %.5f = %.5f = |X.b| / |e| = %.5f / %.5f" \
#                   % (snr, np.linalg.norm(np.dot(X, x * beta)) \
#                                           / np.linalg.norm(e),
#                      np.linalg.norm(np.dot(X, x * beta)), np.linalg.norm(e))

            return (np.linalg.norm(np.dot(X, x * beta)) / np.linalg.norm(e)) \
                    - snr

        snr = bisection_method(f, low=0.0, high=np.sqrt(snr), maxiter=30)

        beta = beta * snr

    X, y = _generate(l, k, g, beta, M, e, A, mu, intercept)

    return X, y, beta


def _generate(l, k, g, beta, M, e, A, mu, intercept):

    p = beta.shape[0]

    if intercept:
        beta_ = beta[1:, :]
        gradL1 = grad_l1(beta_)
        gradL2 = grad_l2_squared(beta_)
        gradGroupTVmu = grad_grouptvmu(beta_, A, mu)
    else:
        gradL1 = grad_l1(beta)
        gradL2 = grad_l2_squared(beta)
        gradGroupTVmu = grad_grouptvmu(beta, A, mu)

    alpha = -(l * gradL1 + k * gradL2 + g * gradGroupTVmu)
    Mte = np.dot(M.T, e)
    if intercept:
        alpha = np.divide(alpha, Mte[1:, :])
    else:
        alpha = np.divide(alpha, Mte)

    X = np.ones(M.shape)
    if intercept:
        for i in range(p - 1):
            X[:, i + 1] = M[:, i + 1] * alpha[i, 0]
    else:
        for i in range(p):
            X[:, i] = M[:, i] * alpha[i, 0]

    y = np.dot(X, beta) - e

    return X, y
